---
title: "R Notebook"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(here)
library(smatr)
library(janitor)
library(skimr)
library(gridExtra)
library(olsrr)
library(purrr)
library(tidyr)

#source(here("R/func/min_cutoff.R"))
```


```{r, warning=FALSE}
biomass_data_taxaclean <- read_csv(here("data/processed/biomass_data_cleannames.csv"))
```

## Pre-cleaning the data

setting minimum mass cut-off (due to higher proportional measurement error at very small mass values) and removing observations when wet mass is less than dry mass (considering these errors in measurement or recording)
```{r}

biomass_data_taxaclean_mass_cutoff <- biomass_data_taxaclean |>
   filter(wm_g >= 0.001, dm_g >=0.001, wm_g >= dm_g)
 
  # filter(wm_g >= 0.001, dm_g >=0.001, afdm_g >=0.001, wm_g >= dm_g, dm_g >= afdm_g) |>
  # select(source, taxa_clean, preservation_method, wm_g, dm_g, afdm_g) |>
  # print(n = Inf)

```

```{r}

#min_cutoff(data = biomass_data_taxaclean, dm_g)

```

filtering frozen samples only
```{r}

biomass_frozen_taxaclean_mass_cutoff <- biomass_data_taxaclean_mass_cutoff |>
  filter(preservation_method == "Frozen")

```

Creating observation number
```{r}

biomass_wobs <- biomass_frozen_taxaclean_mass_cutoff |> 
  mutate(obs_num = row_number())

```

## Identifying outlier candidates

### Cleveland Plots
Creating Cleveland plots for each taxa as a visual tool to help identify observations that are far-removed from the rest.
```{r fig.height=15, fig.width=10}

biomass_wobs |> ggplot(aes(x = wm_g, y = obs_num, label = obs_num)) +
  facet_wrap(~ taxa_clean, ncol = 4, scale = "free") +
  geom_point(colour = "white") +
  geom_text()

```

### Percentage analysis 
Identifying top and bottom 2.5% of the data for each mass variable
```{r}

biomass_frozen_percentiles <- biomass_wobs |>
  mutate(keep_data_wm_percentiles = ifelse(wm_g >= quantile(wm_g, 0.025, na.rm = TRUE) & wm_g <= quantile(wm_g, 0.975, na.rm = TRUE), TRUE, FALSE),
        keep_data_dm_percentiles = ifelse(dm_g >= quantile(dm_g, 0.025, na.rm = TRUE) & dm_g <= quantile(dm_g, 0.975, na.rm = TRUE), TRUE, FALSE))
  
# Checking which observations are identified by the percentile method
biomass_frozen_percentiles |> 
  ungroup() |> 
  filter(keep_data_wm_percentiles == FALSE) |> 
  select(taxa_clean, wm_g, keep_data_wm_percentiles, obs_num) |>
  print(n = Inf)
               
```

### Measures of outlyingness
Calculating measures of outlyingness and associated cutoff values for outlier identification
```{r}

#Nesting by taxa
taxa_nested <- biomass_frozen_percentiles |> 
  select(taxa_clean, wm_g, dm_g, obs_num, keep_data_wm_percentiles, keep_data_dm_percentiles) |> 
  nest(data = c(wm_g, dm_g, obs_num, keep_data_wm_percentiles, keep_data_dm_percentiles))
  
#Calculating measures of outlyingness and associated cutoff values
taxa_nested_outlier_measures <- taxa_nested |> 
  mutate(lm_object = map(.x = data,
                         ~ lm(dm_g ~ wm_g, data = .x)),
         n = map(.x = lm_object,
                 ~ nrow(.x$model)),
         predictors = map(.x = lm_object,
                          ~length(coef(.x))-1),
         leverage = map(.x = lm_object,
                        ~ hatvalues(.x)),
         leverage_cutoff = map2(.x = predictors,
                           .y = n,
                           ~ 2*(.x+1)/.y),
         stud_residual = map(.x = lm_object,
                             ~rstudent(.x)),
         alpha_tdist = map(.x = n,
                           ~0.05/.x),
         df_tdist = map2(.x = n,
                         .y = predictors,
                         ~.x-.y-1),
         critical_t = map2(.x = alpha_tdist,
                           .y = df_tdist,
                           ~qt(.x/2, .y, lower.tail = FALSE, log.p = FALSE)),
         cooks_d = map(.x = lm_object, 
                       ~ cooks.distance(.x)),
         cooks_d_cutoff = map2(.x = predictors,
                          .y = n,
                          ~qf(0.5, .x+1, .y-.x-1, lower.tail = FALSE)),
         dffits = map(.x = lm_object,
                      ~dffits(.x)),
         dffits_cutoff = map2(.x = n,
                              .y = predictors,
                              ~2*sqrt((.y+1)/.x)),
         dfbetas = map(.x = lm_object,
                       ~dfbetas(.x)),
         dfbetas_cutoff = map(.x = n,
                              ~2/sqrt(.x))
  )

```

Creating logical vectors identifying which observations fall outside the cutoff value or range for each measure of outlyingness
```{r}

taxa_nested_outliers_identified <- taxa_nested_outlier_measures |>
  mutate(keep_data_leverage = map2(.x = leverage,
                         .y = leverage_cutoff,
                         ~.x < .y),
         keep_data_stud_resid = map2(.x = stud_residual,
                                     .y = critical_t,
                                     ~.x < .y & .x > -.y),
         keep_data_cook = map2(.x = cooks_d,
                          .y = cooks_d_cutoff,
                          ~.x < .y),
         keep_data_dffits = map2(.x = dffits,
                                 .y = dffits_cutoff,
                                 ~.x < .y & .x > -.y),
         keep_data_dfbetas = map2(.x = dfbetas,
                                  .y = dfbetas_cutoff,
                                  ~.x < .y & .x > -.y))

```

Checking which observations of a particular taxa are identified by different measures of outlyingess
```{r}

taxa_nested_outliers_identified |>
  unnest(cols = c(data, leverage,
  stud_residual, cooks_d, dffits,
  dfbetas, keep_data_leverage, keep_data_stud_resid,
  keep_data_cook, keep_data_dffits, keep_data_dfbetas)) |>
  filter(taxa_clean == "Amphipoda") |>
  select(leverage, keep_data_leverage, obs_num) |>
  arrange(desc(leverage))|> print(n = Inf) |>
  tabyl(keep_data_leverage)

```

Unnesting the data and consolidating all outlier candidates into one vector
```{r}

taxa_unnested <- taxa_nested_outliers_identified |>
  select(taxa_clean, data, keep_data_leverage, keep_data_stud_resid, keep_data_cook, keep_data_dffits, keep_data_dfbetas) |> 
  unnest(cols = c(taxa_clean, data, keep_data_leverage, keep_data_stud_resid, keep_data_cook, keep_data_dffits, keep_data_dfbetas)) 

taxa_unnested_keep_data_merged <- taxa_unnested |>
 # assigning dfbetas values for intercept and slope to separate vectors
   mutate(keep_data_dfbetas_int = keep_data_dfbetas[,1], 
         keep_data_dfbetas_slope = keep_data_dfbetas[,2]) |>
 # Consolidating all outlier candidates into one vector
   mutate(keep_data = ifelse(keep_data_wm_percentiles == TRUE & keep_data_dm_percentiles == TRUE & keep_data_cook == TRUE & keep_data_leverage == TRUE & keep_data_stud_resid == TRUE & keep_data_dffits == TRUE & keep_data_dfbetas_int == TRUE & keep_data_dfbetas_slope == TRUE, TRUE, FALSE)) 

```

### Getting a summary table of counts for all outlier methods 
```{r}

long_data <- taxa_unnested_keep_data_merged |> 
  ungroup() |> 
  select(-c(keep_data_dfbetas)) |> 
  pivot_longer(starts_with("keep"), names_to = "outlier_method") 

long_data |> 
  group_by(outlier_method) |> 
  summarise(n = n(),
            n_FALSE = length(which(value == FALSE)),
            n_TRUE = length(which(value == TRUE)),
            n_NA = length(which(is.na(value))),
            n_add_check = n_FALSE + n_TRUE + n_NA,
            percent_false_or_NA = (n_FALSE+n_NA)/(n_FALSE+n_TRUE+n_NA)*100
  )

```

# Save the data
```{r}
biomass_frozen_outliers_identified <- taxa_unnested_keep_data_merged |>
  select(taxa_clean, wm_g, dm_g, keep_data)

write_csv(biomass_frozen_outliers_identified, here("data/processed/biomass_frozen_outliers_identified.csv"))
```

